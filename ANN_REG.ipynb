{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "USrBZ2srY_aS",
   "metadata": {
    "id": "USrBZ2srY_aS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "add24c9c",
   "metadata": {
    "id": "add24c9c"
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "\n",
    "\n",
    "    def __init__(self, layers=[], learning_rate=0.001, iterations=100):\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "      \n",
    "    def init_weights(self):\n",
    "\n",
    "        np.random.seed(1) # Seed the random number generator\n",
    "        for i in range(1, len(self.layers)):\n",
    "          self.params[\"W\" + str(i)] = np.random.randn(self.layers[i-1], self.layers[i]) \n",
    "          self.params['b' + str(i)]  = np.random.randn(self.layers[i],)\n",
    "       # print(self.params)\n",
    "\n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z)\n",
    "\n",
    "    def dRelu(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "\n",
    "    def eta(self, x):\n",
    "        ETA = 0.0000000001\n",
    "        return np.maximum(x, ETA)\n",
    "\n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z))\n",
    "\n",
    "    def mse(self,y, yhat):\n",
    "        nsample = len(y)\n",
    "        yhat_inv = 1.0 - yhat\n",
    "        y_inv = 1.0 - y\n",
    "        yhat = self.eta(yhat) ## clips value to avoid NaNs in log\n",
    "        yhat_inv = self.eta(yhat_inv) \n",
    "        loss = -1/nsample * (np.sum(np.multiply(np.log(yhat), y) + np.multiply((y_inv), np.log(yhat_inv))))\n",
    "        return loss\n",
    "\n",
    "    def forward_propagation(self):\n",
    "\n",
    "        A = self.X\n",
    "        for i in range(1, len(self.layers)):\n",
    "            d = np.dot(A, self.params['W'+str(i)])\n",
    "            Z = d + self.params['b'+str(i)]\n",
    "            if i == len(self.layers)-1:\n",
    "                A = self.sigmoid(Z)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        yhat = A\n",
    "        loss = self.mse(self.y,yhat)\n",
    "\n",
    "        # save calculated parameters     \n",
    "        self.params['A'] = A\n",
    "\n",
    "        return yhat,loss\n",
    "\n",
    "    def back_propagation(self,yhat):\n",
    "\n",
    "        A = self.params['A']\n",
    "        L = len(self.layers) - 1\n",
    "        y_inv = 1 - self.y\n",
    "        yhat_inv = 1 - yhat\n",
    "\n",
    "\n",
    "        dl_wrt_yhat = np.divide(y_inv, self.eta(yhat_inv)) - np.divide(self.y, self.eta(yhat))\n",
    "        dl_wrt_sig = yhat * (yhat_inv)\n",
    "        dl_wrt_z = [None] * L\n",
    "        dl_wrt_A = [None] * L\n",
    "        dl_wrt_w = [None] * L\n",
    "        dl_wrt_b = [None] * L\n",
    "\n",
    "        dl_wrt_z[L-1] = dl_wrt_yhat * dl_wrt_sig\n",
    "        dl_wrt_A[L-1] = np.dot(dl_wrt_z[L-1], self.params['W'+str(L)].T)\n",
    "        dl_wrt_w[L-1] = np.dot(A.T, dl_wrt_z[L-1])\n",
    "        dl_wrt_b[L-1] = np.sum(dl_wrt_z[L-1], axis=0)\n",
    "\n",
    "        for l in range(L-2, -1, -1):\n",
    "            dl_wrt_z[l] = dl_wrt_A[l+1] * self.dRelu(A)\n",
    "            dl_wrt_A[l] = np.dot(dl_wrt_z[l], self.params['W'+str(l+1)].T)\n",
    "            dl_wrt_w[l] = np.dot(self.params['A'+str(l)].T, dl_wrt_z[l])\n",
    "            dl_wrt_b[l] = np.sum(dl_wrt_z[l], axis=0)\n",
    "            \n",
    "        for i in range(1, L):\n",
    "            self.params['W'+str(i)] -= self.learning_rate * dl_wrt_w[i-1]\n",
    "            self.params['b'+str(i)] -= self.learning_rate * dl_wrt_b[i-1]\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights() # initialize weights and bias\n",
    "\n",
    "        for i in range(self.iterations):\n",
    "            yhat, loss = self.forward_propagation()\n",
    "            self.back_propagation(yhat)\n",
    "            self.loss.append(loss)\n",
    "        print(len(yhat))\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        A = X\n",
    "        for i in range(1, len(self.layers)):\n",
    "          Z = A.dot(self.params['W'+str(i)]) + self.params['b'+str(i)]\n",
    "        \n",
    "          if i == len(self.layers)-1:\n",
    "            A = self.sigmoid(Z)\n",
    "            A = self.relu(Z)\n",
    "        return np.round(A)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "Ug3VUuzSY_Xv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ug3VUuzSY_Xv",
    "outputId": "95a1ecb8-f1ad-41f3-80c3-7ea63844b353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data[:,0:13]\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "nxxCsb6wY_VS",
   "metadata": {
    "id": "nxxCsb6wY_VS"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twgIUfEzHJnf",
   "metadata": {
    "id": "twgIUfEzHJnf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "TFzYpIFZY_S3",
   "metadata": {
    "id": "TFzYpIFZY_S3"
   },
   "outputs": [],
   "source": [
    "input = len(X[0]) # no. of features\n",
    "output = len(y) #no. of classes\n",
    "\n",
    "layers = [input, 3, 8, output] # no. of nodes \n",
    "model = NeuralNet(layers = layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "UUz994KBY_QN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "UUz994KBY_QN",
    "outputId": "3e1306d1-48ce-4727-9348-736bfc555871"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-ac5b9c611f24>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-ac5b9c611f24>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (354,13) and (3,8) not aligned: 13 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fNTxOesiaBlp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "fNTxOesiaBlp",
    "outputId": "ebcc13df-7a28-4a2c-8f68-e8fb8b780680"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b808456a8146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Predicting R2 Score the Train set results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mr2_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-bd8fe2d5081c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m           \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (354,13) and (3,8) not aligned: 13 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Predicting R2 Score the Train set results\n",
    "y_pred = model.predict(X_train)\n",
    "r2_score = r2_score(y_train, y_pred)\n",
    "\n",
    "# Predicting R2 Score the Test set results\n",
    "y_pred_test = model.predict(X_test)\n",
    "r2_score = r2_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MvZiSL7LaBjH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvZiSL7LaBjH",
    "outputId": "cc0a8b21-b575-48b4-8907-2802cb4a5584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
